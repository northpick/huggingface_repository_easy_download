# -*- coding: utf-8 -*-
"""
HFä¸‹è½½å™¨ - ä¿®å¤ç‰ˆ
- ä¿®å¤å¤§æ–‡ä»¶æ£€æµ‹é—®é¢˜
- æ·»åŠ ç¼“å­˜æ¸…ç†é€‰é¡¹
- ç›´æ¥ä¸‹è½½åˆ°å½“å‰ç›®å½•
"""

import re
import sys
import os
import warnings
import time
import shutil
import json
from pathlib import Path
from huggingface_hub import snapshot_download, HfApi, HfFileSystem
import concurrent.futures

# å¿½ç•¥ SSL/HEAD è­¦å‘Š
warnings.filterwarnings("ignore", message=".*SSL: UNEXPECTED_EOF_WHILE_READING.*")
warnings.filterwarnings("ignore", message=".*resume_download.*")

# åˆå§‹åŒ–æ–‡ä»¶ç³»ç»Ÿ
hffs = HfFileSystem()

def parse_url(u):
    """è§£æURLï¼Œè·å–ä»“åº“IDã€ä»“åº“åç§°å’Œå­æ–‡ä»¶å¤¹è·¯å¾„"""
    u = u.strip()
    # ç§»é™¤æœ«å°¾çš„æ–œæ å’Œå¯èƒ½çš„æŸ¥è¯¢å‚æ•°
    u = u.split('?')[0].rstrip("/")
    
    # æå–ä»“åº“ä¿¡æ¯
    m = re.search(r"huggingface\.co/([^/]+)/([^/]+)", u)
    if not m:
        return None, None, None
    
    repo_owner = m.group(1)
    repo_name = m.group(2)
    repo_id = f"{repo_owner}/{repo_name}"
    
    # æå–å­æ–‡ä»¶å¤¹è·¯å¾„
    sub_match = re.search(r"/tree/main/([^?#]+)", u)
    subfolder = sub_match.group(1).rstrip("/") if sub_match else None
    
    return repo_id, repo_name, subfolder

def get_all_files_recursive(repo_id, subfolder=None):
    """é€’å½’è·å–ä»“åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    all_files = []
    
    try:
        # æ„å»ºä»“åº“è·¯å¾„
        base_path = f"{repo_id}@main"
        if subfolder:
            base_path = f"{base_path}/{subfolder}"
        
        print(f"æ­£åœ¨æ‰«æä»“åº“: {base_path}")
        
        # ä½¿ç”¨é€’å½’æ–¹å¼è·å–æ–‡ä»¶åˆ—è¡¨
        def scan_directory(path):
            try:
                items = hffs.ls(path, detail=True)
                
                for item in items:
                    # æ„å»ºç›¸å¯¹è·¯å¾„
                    if item["name"].startswith(f"{repo_id}@main/"):
                        relative_path = item["name"][len(f"{repo_id}@main/"):]
                    else:
                        relative_path = item["name"]
                    
                    if item["type"] == "file":
                        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                        all_files.append({
                            "path": relative_path,
                            "full_path": item["name"],
                            "size": item.get("size", 0),
                            "type": "file"
                        })
                    elif item["type"] == "directory":
                        # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’æ‰«æ
                        scan_directory(item["name"])
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•æ‰«æç›®å½• {path}: {e}")
        
        # å¼€å§‹æ‰«æ
        scan_directory(base_path)
        
        print(f"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        return all_files
    except Exception as e:
        print(f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return []

def get_files_from_api(repo_id, subfolder=None):
    """é€šè¿‡APIè·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆå¤‡é€‰æ–¹æ³•ï¼‰"""
    try:
        api = HfApi()
        
        # è·å–ä»“åº“ä¿¡æ¯
        repo_info = api.repo_info(repo_id, repo_type="model")
        
        # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
        all_files = []
        
        # æ„å»ºå‰ç¼€
        prefix = subfolder if subfolder else ""
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = api.list_repo_files(repo_id, repo_type="model")
        
        for file_path in files:
            # å¦‚æœæŒ‡å®šäº†å­æ–‡ä»¶å¤¹ï¼Œåªå¤„ç†è¯¥æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶
            if prefix:
                if not file_path.startswith(prefix):
                    continue
                # ç§»é™¤å­æ–‡ä»¶å¤¹å‰ç¼€
                relative_path = file_path[len(prefix):].lstrip('/')
                if not relative_path:  # å¦‚æœæ˜¯å­æ–‡ä»¶å¤¹æœ¬èº«ï¼Œè·³è¿‡
                    continue
            else:
                relative_path = file_path
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            try:
                file_info = hffs.info(f"{repo_id}@main/{file_path}")
                all_files.append({
                    "path": relative_path if relative_path else file_path,
                    "full_path": file_path,
                    "size": file_info.get("size", 0),
                    "type": "file"
                })
            except:
                all_files.append({
                    "path": relative_path if relative_path else file_path,
                    "full_path": file_path,
                    "size": 0,
                    "type": "file"
                })
        
        print(f"é€šè¿‡APIæ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        return all_files
    except Exception as e:
        print(f"APIè·å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes == 0:
        return "0 B"
    
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    unit_index = 0
    
    while size_bytes >= 1024 and unit_index < len(units) - 1:
        size_bytes /= 1024
        unit_index += 1
    
    if unit_index == 0:
        return f"{size_bytes} B"
    elif unit_index == 1:
        return f"{size_bytes:.1f} KB"
    elif unit_index == 2:
        return f"{size_bytes:.1f} MB"
    else:
        return f"{size_bytes:.2f} {units[unit_index]}"

def download_small_files(repo_id, target_dir, subfolder=None):
    """ä¸‹è½½å°æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
    try:
        print("æ­£åœ¨ä¸‹è½½å°æ–‡ä»¶...")
        
        # å®šä¹‰å°æ–‡ä»¶æ¨¡å¼ï¼ˆè¿™äº›æ‰©å±•åçš„æ–‡ä»¶é€šå¸¸è¾ƒå°ï¼‰
        allow_patterns = [
            "*.json", "*.txt", "*.yaml", "*.yml", "*.md", 
            "*.py", "*.cpp", "*.c", "*.h", "*.hpp",
            "*.html", "*.css", "*.js", "*.xml", "*.ini", "*.cfg",
            "*.png", "*.jpg", "*.jpeg", "*.gif", "*.bmp", "*.webp",
            "*.csv", "*.tsv", "*.log",
            "tokenizer/*", "scheduler/*", "feature_extractor/*",
            "config.json", "model_index.json", "preprocessor_config.json",
            "*.vocab", "*.merges", "*.model"
        ]
        
        # å®šä¹‰å¤§æ–‡ä»¶æ¨¡å¼ï¼ˆè¿™äº›æ–‡ä»¶ä¸ä¸‹è½½ï¼Œåªç”Ÿæˆé“¾æ¥ï¼‰
        ignore_patterns = [
            "*.safetensors", "*.bin", "*.pt", "*.ckpt", "*.pth", 
            "*.msgpack", "*.h5", "*.gguf", "*.onnx", "*.tflite",
            "*/pytorch_model*.bin", "*/model*.safetensors",
            "*/diffusion_pytorch_model*.bin"
        ]
        
        # å¦‚æœæœ‰å­æ–‡ä»¶å¤¹ï¼Œè°ƒæ•´æ¨¡å¼
        if subfolder:
            allow_patterns = [f"{subfolder}/{p}" for p in allow_patterns]
            ignore_patterns = [f"{subfolder}/{p}" for p in ignore_patterns]
        
        # ä¸‹è½½åˆ°ç›®æ ‡ç›®å½•
        cache_path = snapshot_download(
            repo_id=repo_id,
            repo_type="model",
            local_dir=target_dir,  # ç›´æ¥ä¸‹è½½åˆ°ç›®æ ‡ç›®å½•
            local_dir_use_symlinks=False,
            allow_patterns=allow_patterns,
            ignore_patterns=ignore_patterns,
            resume_download=True,
            max_workers=4,
            tqdm_class=None
        )
        
        print("âœ… å°æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        return True
    except Exception as e:
        print(f"ä¸‹è½½å°æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def classify_files_by_size(file_list, size_threshold_mb=50):
    """æŒ‰å¤§å°åˆ†ç±»æ–‡ä»¶"""
    small_files = []
    big_files = []
    
    for file_info in file_list:
        file_size = file_info["size"]
        file_path = file_info["path"]
        
        # å¤§å°åˆ¤æ–­ï¼šå°äºé˜ˆå€¼ä¸ºå°æ–‡ä»¶ï¼Œå¦åˆ™ä¸ºå¤§æ–‡ä»¶
        if file_size < size_threshold_mb * 1024 * 1024:  # è½¬æ¢ä¸ºå­—èŠ‚
            small_files.append(file_info)
        else:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼Œç¡®ä¿ä¸æ˜¯å°æ–‡ä»¶ç±»å‹
            small_extensions = ['.json', '.txt', '.yaml', '.yml', '.md', '.py', 
                               '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.csv']
            if any(file_path.lower().endswith(ext) for ext in small_extensions):
                # å³ä½¿æ–‡ä»¶å¤§ï¼Œä½†æ‰©å±•åæ˜¯å°æ–‡ä»¶ç±»å‹ï¼Œä¹Ÿå½“ä½œå°æ–‡ä»¶å¤„ç†
                small_files.append(file_info)
            else:
                big_files.append(file_info)
    
    return small_files, big_files

def generate_big_file_links(repo_id, repo_name, big_files, target_dir, subfolder=None):
    """ç”Ÿæˆå¤§æ–‡ä»¶é“¾æ¥"""
    if not big_files:
        print("æ²¡æœ‰æ£€æµ‹åˆ°å¤§æ–‡ä»¶")
        return None
    
    print(f"æ£€æµ‹åˆ° {len(big_files)} ä¸ªå¤§æ–‡ä»¶:")
    
    # åˆ›å»ºé“¾æ¥æ–‡ä»¶å
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    if subfolder:
        safe_subfolder = subfolder.replace('/', '_').replace('\\', '_')
        link_filename = f"{repo_name}_{safe_subfolder}_å¤§æ–‡ä»¶_{timestamp}.txt"
    else:
        link_filename = f"{repo_name}_å¤§æ–‡ä»¶_{timestamp}.txt"
    
    link_file = target_dir / link_filename
    
    try:
        with open(link_file, "w", encoding="utf-8") as f:
            f.write(f"HuggingFace å¤§æ–‡ä»¶ä¸‹è½½é“¾æ¥\n")
            f.write("=" * 70 + "\n")
            f.write(f"ä»“åº“: {repo_id}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            if subfolder:
                f.write(f"å­æ–‡ä»¶å¤¹: {subfolder}\n")
            f.write(f"å¤§æ–‡ä»¶æ•°é‡: {len(big_files)} ä¸ª\n")
            f.write("=" * 70 + "\n\n")
            
            total_size = 0
            for i, file_info in enumerate(big_files, 1):
                file_path = file_info["path"]
                file_size = file_info["size"]
                total_size += file_size
                
                # ç”Ÿæˆä¸‹è½½é“¾æ¥
                download_url = f"https://huggingface.co/{repo_id}/resolve/main/{file_path}"
                if subfolder and not file_path.startswith(subfolder):
                    # ç¡®ä¿æ–‡ä»¶è·¯å¾„åŒ…å«å­æ–‡ä»¶å¤¹
                    full_path = f"{subfolder}/{file_path}" if subfolder else file_path
                    download_url = f"https://huggingface.co/{repo_id}/resolve/main/{full_path}"
                
                f.write(f"ã€æ–‡ä»¶ {i}ã€‘\n")
                f.write(f"æ–‡ä»¶å: {Path(file_path).name}\n")
                f.write(f"è·¯å¾„: {file_path}\n")
                f.write(f"å¤§å°: {format_file_size(file_size)}\n")
                f.write(f"ä¸‹è½½é“¾æ¥: {download_url}\n")
                f.write("-" * 70 + "\n\n")
            
            f.write(f"\næ€»è®¡: {len(big_files)} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_size)}\n")
        
        print(f"âœ… å·²ç”Ÿæˆå¤§æ–‡ä»¶é“¾æ¥æ–‡ä»¶: {link_file}")
        print(f"   åŒ…å« {len(big_files)} ä¸ªå¤§æ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_size)}")
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("\nğŸ“ å¤§æ–‡ä»¶åˆ—è¡¨:")
        for i, file_info in enumerate(big_files[:20], 1):
            file_name = Path(file_info["path"]).name
            print(f"   {i:2d}. {file_name[:50]:50} {format_file_size(file_info['size']):>10}")
        
        if len(big_files) > 20:
            print(f"   ... è¿˜æœ‰ {len(big_files) - 20} ä¸ªæ–‡ä»¶")
        
        return link_file
    except Exception as e:
        print(f"ç”Ÿæˆé“¾æ¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def ask_for_cache_cleanup(repo_folder):
    """è¯¢é—®æ˜¯å¦æ¸…ç†ç¼“å­˜"""
    print("\n" + "=" * 70)
    print("ç¼“å­˜æ¸…ç†é€‰é¡¹")
    print("=" * 70)
    
    # æ£€æŸ¥.cacheæ–‡ä»¶å¤¹
    cache_folder = repo_folder / ".cache"
    
    if cache_folder.exists():
        # è®¡ç®—ç¼“å­˜å¤§å°
        cache_size = 0
        try:
            for root, dirs, files in os.walk(cache_folder):
                for file in files:
                    try:
                        cache_size += os.path.getsize(os.path.join(root, file))
                    except:
                        pass
        except:
            cache_size = 0
        
        print(f"æ£€æµ‹åˆ°ç¼“å­˜æ–‡ä»¶å¤¹: {cache_folder}")
        print(f"ç¼“å­˜å¤§å°: {format_file_size(cache_size)}")
        
        # è¯¢é—®ç”¨æˆ·
        while True:
            choice = input("\næ˜¯å¦åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤¹ï¼Ÿ(y/n): ").strip().lower()
            if choice in ['y', 'yes', 'æ˜¯']:
                try:
                    shutil.rmtree(cache_folder)
                    print("âœ… ç¼“å­˜æ–‡ä»¶å¤¹å·²åˆ é™¤")
                except Exception as e:
                    print(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
                break
            elif choice in ['n', 'no', 'å¦']:
                print("âœ… å·²ä¿ç•™ç¼“å­˜æ–‡ä»¶å¤¹")
                break
            else:
                print("è¯·è¾“å…¥ y/n æˆ– æ˜¯/å¦")
    else:
        print("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶å¤¹")

def main():
    print("=" * 70)
    print("HFä¸‹è½½å™¨ - ä¿®å¤ç‰ˆ".center(70))
    print("=" * 70)
    
    # æµ‹è¯•URLï¼ˆç”¨äºè°ƒè¯•ï¼‰
    # url = "https://huggingface.co/hustvl/vitmatte-small-composition-1k/tree/main"
    url = input("\nè¯·è¾“å…¥HuggingFaceä»“åº“é“¾æ¥ â†’ ").strip()
    
    if not url:
        print("é”™è¯¯: è¯·è¾“å…¥æœ‰æ•ˆçš„é“¾æ¥!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    # è§£æURL
    repo_id, repo_name, subfolder = parse_url(url)
    
    if not repo_id:
        print("é”™è¯¯: æ— æ³•è§£æé“¾æ¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ ¼å¼!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    print(f"\nâœ… ä»“åº“ä¿¡æ¯:")
    print(f"   ä»“åº“ID: {repo_id}")
    print(f"   ä»“åº“åç§°: {repo_name}")
    if subfolder:
        print(f"   å­æ–‡ä»¶å¤¹: {subfolder}")
    else:
        print(f"   å­æ–‡ä»¶å¤¹: æ ¹ç›®å½•")
    
    # åˆ›å»ºä»“åº“æ–‡ä»¶å¤¹
    repo_folder = Path.cwd() / repo_name
    try:
        repo_folder.mkdir(exist_ok=True)
        print(f"âœ… å·²åˆ›å»ºæ–‡ä»¶å¤¹: {repo_folder}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {e}")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    # æ­¥éª¤1: è·å–æ–‡ä»¶åˆ—è¡¨
    print("\n" + "=" * 70)
    print("æ­¥éª¤1: æ‰«æä»“åº“æ–‡ä»¶")
    print("=" * 70)
    
    # å°è¯•å¤šç§æ–¹æ³•è·å–æ–‡ä»¶åˆ—è¡¨
    all_files = []
    
    print("å°è¯•æ–¹æ³•1: é€’å½’æ‰«æ...")
    all_files = get_all_files_recursive(repo_id, subfolder)
    
    if not all_files:
        print("\næ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2: ä½¿ç”¨API...")
        all_files = get_files_from_api(repo_id, subfolder)
    
    if not all_files:
        print("âŒ æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé“¾æ¥æœ‰æ•ˆæ€§!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    # æ˜¾ç¤ºæ–‡ä»¶ç»Ÿè®¡
    print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
    print(f"   æ€»è®¡: {len(all_files)} ä¸ªæ–‡ä»¶")
    
    # æŒ‰å¤§å°åˆ†ç±»æ–‡ä»¶
    small_files, big_files = classify_files_by_size(all_files, size_threshold_mb=50)
    
    print(f"   å°æ–‡ä»¶ï¼ˆ<50MBï¼‰: {len(small_files)} ä¸ª")
    print(f"   å¤§æ–‡ä»¶ï¼ˆâ‰¥50MBï¼‰: {len(big_files)} ä¸ª")
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°åˆ†å¸ƒ
    if all_files:
        sizes = [f["size"] for f in all_files]
        max_size = max(sizes) if sizes else 0
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        print(f"   æœ€å¤§æ–‡ä»¶: {format_file_size(max_size)}")
        print(f"   å¹³å‡å¤§å°: {format_file_size(avg_size)}")
    
    # æ­¥éª¤2: ä¸‹è½½å°æ–‡ä»¶
    print("\n" + "=" * 70)
    print("æ­¥éª¤2: ä¸‹è½½å°æ–‡ä»¶")
    print("=" * 70)
    
    if small_files:
        total_small_size = sum(f["size"] for f in small_files)
        print(f"æ­£åœ¨ä¸‹è½½ {len(small_files)} ä¸ªå°æ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_small_size)}")
        
        # ä¸‹è½½å°æ–‡ä»¶
        success = download_small_files(repo_id, repo_folder, subfolder)
        
        if success:
            # æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶
            downloaded_files = []
            for root, dirs, files in os.walk(repo_folder):
                for file in files:
                    if file != ".gitattributes":  # å¿½ç•¥.gitattributesæ–‡ä»¶
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, repo_folder)
                        downloaded_files.append(rel_path)
            
            print(f"âœ… å·²ä¸‹è½½ {len(downloaded_files)} ä¸ªæ–‡ä»¶åˆ°: {repo_folder}")
            
            # æ˜¾ç¤ºä¸‹è½½çš„æ–‡ä»¶
            if downloaded_files:
                print("\nğŸ“„ å·²ä¸‹è½½çš„æ–‡ä»¶:")
                for i, file in enumerate(downloaded_files[:10], 1):
                    print(f"   {i:2d}. {file}")
                if len(downloaded_files) > 10:
                    print(f"   ... è¿˜æœ‰ {len(downloaded_files) - 10} ä¸ªæ–‡ä»¶")
        else:
            print("âš ï¸  å°æ–‡ä»¶ä¸‹è½½å¯èƒ½ä¸å®Œæ•´ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
    else:
        print("æ²¡æœ‰å°æ–‡ä»¶éœ€è¦ä¸‹è½½")
    
    # æ­¥éª¤3: ç”Ÿæˆå¤§æ–‡ä»¶é“¾æ¥
    print("\n" + "=" * 70)
    print("æ­¥éª¤3: ç”Ÿæˆå¤§æ–‡ä»¶ä¸‹è½½é“¾æ¥")
    print("=" * 70)
    
    if big_files:
        total_big_size = sum(f["size"] for f in big_files)
        print(f"æ£€æµ‹åˆ° {len(big_files)} ä¸ªå¤§æ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_big_size)}")
        
        link_file = generate_big_file_links(repo_id, repo_name, big_files, repo_folder, subfolder)
    else:
        print("âœ… æ²¡æœ‰å¤§æ–‡ä»¶éœ€è¦ç”Ÿæˆé“¾æ¥")
    
    # æ­¥éª¤4: ç¼“å­˜æ¸…ç†
    ask_for_cache_cleanup(repo_folder)
    
    # å®Œæˆæç¤º
    print("\n" + "=" * 70)
    print("ä¸‹è½½ä»»åŠ¡å®Œæˆ!".center(70))
    print("=" * 70)
    
    print(f"\nğŸ“‚ æ–‡ä»¶å¤¹ä½ç½®: {repo_folder}")
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤¹å†…å®¹ç»Ÿè®¡
    print(f"\nğŸ“Š æ–‡ä»¶å¤¹å†…å®¹ç»Ÿè®¡:")
    try:
        file_count = 0
        dir_count = 0
        total_size = 0
        
        for root, dirs, files in os.walk(repo_folder):
            dir_count += len(dirs)
            for file in files:
                if file.endswith('.txt') and 'å¤§æ–‡ä»¶' in file:
                    continue  # ä¸ç»Ÿè®¡é“¾æ¥æ–‡ä»¶
                try:
                    file_path = os.path.join(root, file)
                    total_size += os.path.getsize(file_path)
                    file_count += 1
                except:
                    pass
        
        print(f"   æ–‡ä»¶æ•°é‡: {file_count} ä¸ª")
        print(f"   æ–‡ä»¶å¤¹æ•°é‡: {dir_count} ä¸ª")
        print(f"   æ€»å¤§å°: {format_file_size(total_size)}")
    except:
        pass
    
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("   1. å°æ–‡ä»¶å·²ä¸‹è½½åˆ°ä¸Šè¿°æ–‡ä»¶å¤¹")
    
    if big_files:
        print("   2. å¤§æ–‡ä»¶é“¾æ¥å·²ç”Ÿæˆåˆ°txtæ–‡ä»¶ä¸­")
        print("   3. è¯·ä½¿ç”¨ä¸‹è½½å·¥å…·ï¼ˆIDMã€è¿…é›·ç­‰ï¼‰ä¸‹è½½å¤§æ–‡ä»¶")
    
    print("\n" + "=" * 70)
    
    input("\næŒ‰å›è½¦é”®é€€å‡ºç¨‹åº...")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œç¨‹åºé€€å‡º")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)# -*- coding: utf-8 -*-
"""
HFä¸‹è½½å™¨ - ä¿®å¤ç‰ˆ
- ä¿®å¤å¤§æ–‡ä»¶æ£€æµ‹é—®é¢˜
- æ·»åŠ ç¼“å­˜æ¸…ç†é€‰é¡¹
- ç›´æ¥ä¸‹è½½åˆ°å½“å‰ç›®å½•
"""

import re
import sys
import os
import warnings
import time
import shutil
import json
from pathlib import Path
from huggingface_hub import snapshot_download, HfApi, HfFileSystem
import concurrent.futures

# å¿½ç•¥ SSL/HEAD è­¦å‘Š
warnings.filterwarnings("ignore", message=".*SSL: UNEXPECTED_EOF_WHILE_READING.*")
warnings.filterwarnings("ignore", message=".*resume_download.*")

# åˆå§‹åŒ–æ–‡ä»¶ç³»ç»Ÿ
hffs = HfFileSystem()

def parse_url(u):
    """è§£æURLï¼Œè·å–ä»“åº“IDã€ä»“åº“åç§°å’Œå­æ–‡ä»¶å¤¹è·¯å¾„"""
    u = u.strip()
    # ç§»é™¤æœ«å°¾çš„æ–œæ å’Œå¯èƒ½çš„æŸ¥è¯¢å‚æ•°
    u = u.split('?')[0].rstrip("/")
    
    # æå–ä»“åº“ä¿¡æ¯
    m = re.search(r"huggingface\.co/([^/]+)/([^/]+)", u)
    if not m:
        return None, None, None
    
    repo_owner = m.group(1)
    repo_name = m.group(2)
    repo_id = f"{repo_owner}/{repo_name}"
    
    # æå–å­æ–‡ä»¶å¤¹è·¯å¾„
    sub_match = re.search(r"/tree/main/([^?#]+)", u)
    subfolder = sub_match.group(1).rstrip("/") if sub_match else None
    
    return repo_id, repo_name, subfolder

def get_all_files_recursive(repo_id, subfolder=None):
    """é€’å½’è·å–ä»“åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    all_files = []
    
    try:
        # æ„å»ºä»“åº“è·¯å¾„
        base_path = f"{repo_id}@main"
        if subfolder:
            base_path = f"{base_path}/{subfolder}"
        
        print(f"æ­£åœ¨æ‰«æä»“åº“: {base_path}")
        
        # ä½¿ç”¨é€’å½’æ–¹å¼è·å–æ–‡ä»¶åˆ—è¡¨
        def scan_directory(path):
            try:
                items = hffs.ls(path, detail=True)
                
                for item in items:
                    # æ„å»ºç›¸å¯¹è·¯å¾„
                    if item["name"].startswith(f"{repo_id}@main/"):
                        relative_path = item["name"][len(f"{repo_id}@main/"):]
                    else:
                        relative_path = item["name"]
                    
                    if item["type"] == "file":
                        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                        all_files.append({
                            "path": relative_path,
                            "full_path": item["name"],
                            "size": item.get("size", 0),
                            "type": "file"
                        })
                    elif item["type"] == "directory":
                        # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’æ‰«æ
                        scan_directory(item["name"])
            except Exception as e:
                print(f"  è­¦å‘Š: æ— æ³•æ‰«æç›®å½• {path}: {e}")
        
        # å¼€å§‹æ‰«æ
        scan_directory(base_path)
        
        print(f"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        return all_files
    except Exception as e:
        print(f"è·å–æ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return []

def get_files_from_api(repo_id, subfolder=None):
    """é€šè¿‡APIè·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆå¤‡é€‰æ–¹æ³•ï¼‰"""
    try:
        api = HfApi()
        
        # è·å–ä»“åº“ä¿¡æ¯
        repo_info = api.repo_info(repo_id, repo_type="model")
        
        # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
        all_files = []
        
        # æ„å»ºå‰ç¼€
        prefix = subfolder if subfolder else ""
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        files = api.list_repo_files(repo_id, repo_type="model")
        
        for file_path in files:
            # å¦‚æœæŒ‡å®šäº†å­æ–‡ä»¶å¤¹ï¼Œåªå¤„ç†è¯¥æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶
            if prefix:
                if not file_path.startswith(prefix):
                    continue
                # ç§»é™¤å­æ–‡ä»¶å¤¹å‰ç¼€
                relative_path = file_path[len(prefix):].lstrip('/')
                if not relative_path:  # å¦‚æœæ˜¯å­æ–‡ä»¶å¤¹æœ¬èº«ï¼Œè·³è¿‡
                    continue
            else:
                relative_path = file_path
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            try:
                file_info = hffs.info(f"{repo_id}@main/{file_path}")
                all_files.append({
                    "path": relative_path if relative_path else file_path,
                    "full_path": file_path,
                    "size": file_info.get("size", 0),
                    "type": "file"
                })
            except:
                all_files.append({
                    "path": relative_path if relative_path else file_path,
                    "full_path": file_path,
                    "size": 0,
                    "type": "file"
                })
        
        print(f"é€šè¿‡APIæ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        return all_files
    except Exception as e:
        print(f"APIè·å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes == 0:
        return "0 B"
    
    units = ['B', 'KB', 'MB', 'GB', 'TB']
    unit_index = 0
    
    while size_bytes >= 1024 and unit_index < len(units) - 1:
        size_bytes /= 1024
        unit_index += 1
    
    if unit_index == 0:
        return f"{size_bytes} B"
    elif unit_index == 1:
        return f"{size_bytes:.1f} KB"
    elif unit_index == 2:
        return f"{size_bytes:.1f} MB"
    else:
        return f"{size_bytes:.2f} {units[unit_index]}"

def download_small_files(repo_id, target_dir, subfolder=None):
    """ä¸‹è½½å°æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
    try:
        print("æ­£åœ¨ä¸‹è½½å°æ–‡ä»¶...")
        
        # å®šä¹‰å°æ–‡ä»¶æ¨¡å¼ï¼ˆè¿™äº›æ‰©å±•åçš„æ–‡ä»¶é€šå¸¸è¾ƒå°ï¼‰
        allow_patterns = [
            "*.json", "*.txt", "*.yaml", "*.yml", "*.md", 
            "*.py", "*.cpp", "*.c", "*.h", "*.hpp",
            "*.html", "*.css", "*.js", "*.xml", "*.ini", "*.cfg",
            "*.png", "*.jpg", "*.jpeg", "*.gif", "*.bmp", "*.webp",
            "*.csv", "*.tsv", "*.log",
            "tokenizer/*", "scheduler/*", "feature_extractor/*",
            "config.json", "model_index.json", "preprocessor_config.json",
            "*.vocab", "*.merges", "*.model"
        ]
        
        # å®šä¹‰å¤§æ–‡ä»¶æ¨¡å¼ï¼ˆè¿™äº›æ–‡ä»¶ä¸ä¸‹è½½ï¼Œåªç”Ÿæˆé“¾æ¥ï¼‰
        ignore_patterns = [
            "*.safetensors", "*.bin", "*.pt", "*.ckpt", "*.pth", 
            "*.msgpack", "*.h5", "*.gguf", "*.onnx", "*.tflite",
            "*/pytorch_model*.bin", "*/model*.safetensors",
            "*/diffusion_pytorch_model*.bin"
        ]
        
        # å¦‚æœæœ‰å­æ–‡ä»¶å¤¹ï¼Œè°ƒæ•´æ¨¡å¼
        if subfolder:
            allow_patterns = [f"{subfolder}/{p}" for p in allow_patterns]
            ignore_patterns = [f"{subfolder}/{p}" for p in ignore_patterns]
        
        # ä¸‹è½½åˆ°ç›®æ ‡ç›®å½•
        cache_path = snapshot_download(
            repo_id=repo_id,
            repo_type="model",
            local_dir=target_dir,  # ç›´æ¥ä¸‹è½½åˆ°ç›®æ ‡ç›®å½•
            local_dir_use_symlinks=False,
            allow_patterns=allow_patterns,
            ignore_patterns=ignore_patterns,
            resume_download=True,
            max_workers=4,
            tqdm_class=None
        )
        
        print("âœ… å°æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        return True
    except Exception as e:
        print(f"ä¸‹è½½å°æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def classify_files_by_size(file_list, size_threshold_mb=50):
    """æŒ‰å¤§å°åˆ†ç±»æ–‡ä»¶"""
    small_files = []
    big_files = []
    
    for file_info in file_list:
        file_size = file_info["size"]
        file_path = file_info["path"]
        
        # å¤§å°åˆ¤æ–­ï¼šå°äºé˜ˆå€¼ä¸ºå°æ–‡ä»¶ï¼Œå¦åˆ™ä¸ºå¤§æ–‡ä»¶
        if file_size < size_threshold_mb * 1024 * 1024:  # è½¬æ¢ä¸ºå­—èŠ‚
            small_files.append(file_info)
        else:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼Œç¡®ä¿ä¸æ˜¯å°æ–‡ä»¶ç±»å‹
            small_extensions = ['.json', '.txt', '.yaml', '.yml', '.md', '.py', 
                               '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.csv']
            if any(file_path.lower().endswith(ext) for ext in small_extensions):
                # å³ä½¿æ–‡ä»¶å¤§ï¼Œä½†æ‰©å±•åæ˜¯å°æ–‡ä»¶ç±»å‹ï¼Œä¹Ÿå½“ä½œå°æ–‡ä»¶å¤„ç†
                small_files.append(file_info)
            else:
                big_files.append(file_info)
    
    return small_files, big_files

def generate_big_file_links(repo_id, repo_name, big_files, target_dir, subfolder=None):
    """ç”Ÿæˆå¤§æ–‡ä»¶é“¾æ¥"""
    if not big_files:
        print("æ²¡æœ‰æ£€æµ‹åˆ°å¤§æ–‡ä»¶")
        return None
    
    print(f"æ£€æµ‹åˆ° {len(big_files)} ä¸ªå¤§æ–‡ä»¶:")
    
    # åˆ›å»ºé“¾æ¥æ–‡ä»¶å
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    if subfolder:
        safe_subfolder = subfolder.replace('/', '_').replace('\\', '_')
        link_filename = f"{repo_name}_{safe_subfolder}_å¤§æ–‡ä»¶_{timestamp}.txt"
    else:
        link_filename = f"{repo_name}_å¤§æ–‡ä»¶_{timestamp}.txt"
    
    link_file = target_dir / link_filename
    
    try:
        with open(link_file, "w", encoding="utf-8") as f:
            f.write(f"HuggingFace å¤§æ–‡ä»¶ä¸‹è½½é“¾æ¥\n")
            f.write("=" * 70 + "\n")
            f.write(f"ä»“åº“: {repo_id}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            if subfolder:
                f.write(f"å­æ–‡ä»¶å¤¹: {subfolder}\n")
            f.write(f"å¤§æ–‡ä»¶æ•°é‡: {len(big_files)} ä¸ª\n")
            f.write("=" * 70 + "\n\n")
            
            total_size = 0
            for i, file_info in enumerate(big_files, 1):
                file_path = file_info["path"]
                file_size = file_info["size"]
                total_size += file_size
                
                # ç”Ÿæˆä¸‹è½½é“¾æ¥
                download_url = f"https://huggingface.co/{repo_id}/resolve/main/{file_path}"
                if subfolder and not file_path.startswith(subfolder):
                    # ç¡®ä¿æ–‡ä»¶è·¯å¾„åŒ…å«å­æ–‡ä»¶å¤¹
                    full_path = f"{subfolder}/{file_path}" if subfolder else file_path
                    download_url = f"https://huggingface.co/{repo_id}/resolve/main/{full_path}"
                
                f.write(f"ã€æ–‡ä»¶ {i}ã€‘\n")
                f.write(f"æ–‡ä»¶å: {Path(file_path).name}\n")
                f.write(f"è·¯å¾„: {file_path}\n")
                f.write(f"å¤§å°: {format_file_size(file_size)}\n")
                f.write(f"ä¸‹è½½é“¾æ¥: {download_url}\n")
                f.write("-" * 70 + "\n\n")
            
            f.write(f"\næ€»è®¡: {len(big_files)} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_size)}\n")
        
        print(f"âœ… å·²ç”Ÿæˆå¤§æ–‡ä»¶é“¾æ¥æ–‡ä»¶: {link_file}")
        print(f"   åŒ…å« {len(big_files)} ä¸ªå¤§æ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_size)}")
        
        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        print("\nğŸ“ å¤§æ–‡ä»¶åˆ—è¡¨:")
        for i, file_info in enumerate(big_files[:20], 1):
            file_name = Path(file_info["path"]).name
            print(f"   {i:2d}. {file_name[:50]:50} {format_file_size(file_info['size']):>10}")
        
        if len(big_files) > 20:
            print(f"   ... è¿˜æœ‰ {len(big_files) - 20} ä¸ªæ–‡ä»¶")
        
        return link_file
    except Exception as e:
        print(f"ç”Ÿæˆé“¾æ¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def ask_for_cache_cleanup(repo_folder):
    """è¯¢é—®æ˜¯å¦æ¸…ç†ç¼“å­˜"""
    print("\n" + "=" * 70)
    print("ç¼“å­˜æ¸…ç†é€‰é¡¹")
    print("=" * 70)
    
    # æ£€æŸ¥.cacheæ–‡ä»¶å¤¹
    cache_folder = repo_folder / ".cache"
    
    if cache_folder.exists():
        # è®¡ç®—ç¼“å­˜å¤§å°
        cache_size = 0
        try:
            for root, dirs, files in os.walk(cache_folder):
                for file in files:
                    try:
                        cache_size += os.path.getsize(os.path.join(root, file))
                    except:
                        pass
        except:
            cache_size = 0
        
        print(f"æ£€æµ‹åˆ°ç¼“å­˜æ–‡ä»¶å¤¹: {cache_folder}")
        print(f"ç¼“å­˜å¤§å°: {format_file_size(cache_size)}")
        
        # è¯¢é—®ç”¨æˆ·
        while True:
            choice = input("\næ˜¯å¦åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤¹ï¼Ÿ(y/n): ").strip().lower()
            if choice in ['y', 'yes', 'æ˜¯']:
                try:
                    shutil.rmtree(cache_folder)
                    print("âœ… ç¼“å­˜æ–‡ä»¶å¤¹å·²åˆ é™¤")
                except Exception as e:
                    print(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
                break
            elif choice in ['n', 'no', 'å¦']:
                print("âœ… å·²ä¿ç•™ç¼“å­˜æ–‡ä»¶å¤¹")
                break
            else:
                print("è¯·è¾“å…¥ y/n æˆ– æ˜¯/å¦")
    else:
        print("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶å¤¹")

def main():
    print("=" * 70)
    print("HFä¸‹è½½å™¨ - ä¿®å¤ç‰ˆ".center(70))
    print("=" * 70)
    
    # æµ‹è¯•URLï¼ˆç”¨äºè°ƒè¯•ï¼‰
    # url = "https://huggingface.co/hustvl/vitmatte-small-composition-1k/tree/main"
    url = input("\nè¯·è¾“å…¥HuggingFaceä»“åº“é“¾æ¥ â†’ ").strip()
    
    if not url:
        print("é”™è¯¯: è¯·è¾“å…¥æœ‰æ•ˆçš„é“¾æ¥!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    # è§£æURL
    repo_id, repo_name, subfolder = parse_url(url)
    
    if not repo_id:
        print("é”™è¯¯: æ— æ³•è§£æé“¾æ¥ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ ¼å¼!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    print(f"\nâœ… ä»“åº“ä¿¡æ¯:")
    print(f"   ä»“åº“ID: {repo_id}")
    print(f"   ä»“åº“åç§°: {repo_name}")
    if subfolder:
        print(f"   å­æ–‡ä»¶å¤¹: {subfolder}")
    else:
        print(f"   å­æ–‡ä»¶å¤¹: æ ¹ç›®å½•")
    
    # åˆ›å»ºä»“åº“æ–‡ä»¶å¤¹
    repo_folder = Path.cwd() / repo_name
    try:
        repo_folder.mkdir(exist_ok=True)
        print(f"âœ… å·²åˆ›å»ºæ–‡ä»¶å¤¹: {repo_folder}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥: {e}")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    # æ­¥éª¤1: è·å–æ–‡ä»¶åˆ—è¡¨
    print("\n" + "=" * 70)
    print("æ­¥éª¤1: æ‰«æä»“åº“æ–‡ä»¶")
    print("=" * 70)
    
    # å°è¯•å¤šç§æ–¹æ³•è·å–æ–‡ä»¶åˆ—è¡¨
    all_files = []
    
    print("å°è¯•æ–¹æ³•1: é€’å½’æ‰«æ...")
    all_files = get_all_files_recursive(repo_id, subfolder)
    
    if not all_files:
        print("\næ–¹æ³•1å¤±è´¥ï¼Œå°è¯•æ–¹æ³•2: ä½¿ç”¨API...")
        all_files = get_files_from_api(repo_id, subfolder)
    
    if not all_files:
        print("âŒ æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé“¾æ¥æœ‰æ•ˆæ€§!")
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)
    
    # æ˜¾ç¤ºæ–‡ä»¶ç»Ÿè®¡
    print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
    print(f"   æ€»è®¡: {len(all_files)} ä¸ªæ–‡ä»¶")
    
    # æŒ‰å¤§å°åˆ†ç±»æ–‡ä»¶
    small_files, big_files = classify_files_by_size(all_files, size_threshold_mb=50)
    
    print(f"   å°æ–‡ä»¶ï¼ˆ<50MBï¼‰: {len(small_files)} ä¸ª")
    print(f"   å¤§æ–‡ä»¶ï¼ˆâ‰¥50MBï¼‰: {len(big_files)} ä¸ª")
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°åˆ†å¸ƒ
    if all_files:
        sizes = [f["size"] for f in all_files]
        max_size = max(sizes) if sizes else 0
        avg_size = sum(sizes) / len(sizes) if sizes else 0
        print(f"   æœ€å¤§æ–‡ä»¶: {format_file_size(max_size)}")
        print(f"   å¹³å‡å¤§å°: {format_file_size(avg_size)}")
    
    # æ­¥éª¤2: ä¸‹è½½å°æ–‡ä»¶
    print("\n" + "=" * 70)
    print("æ­¥éª¤2: ä¸‹è½½å°æ–‡ä»¶")
    print("=" * 70)
    
    if small_files:
        total_small_size = sum(f["size"] for f in small_files)
        print(f"æ­£åœ¨ä¸‹è½½ {len(small_files)} ä¸ªå°æ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_small_size)}")
        
        # ä¸‹è½½å°æ–‡ä»¶
        success = download_small_files(repo_id, repo_folder, subfolder)
        
        if success:
            # æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶
            downloaded_files = []
            for root, dirs, files in os.walk(repo_folder):
                for file in files:
                    if file != ".gitattributes":  # å¿½ç•¥.gitattributesæ–‡ä»¶
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, repo_folder)
                        downloaded_files.append(rel_path)
            
            print(f"âœ… å·²ä¸‹è½½ {len(downloaded_files)} ä¸ªæ–‡ä»¶åˆ°: {repo_folder}")
            
            # æ˜¾ç¤ºä¸‹è½½çš„æ–‡ä»¶
            if downloaded_files:
                print("\nğŸ“„ å·²ä¸‹è½½çš„æ–‡ä»¶:")
                for i, file in enumerate(downloaded_files[:10], 1):
                    print(f"   {i:2d}. {file}")
                if len(downloaded_files) > 10:
                    print(f"   ... è¿˜æœ‰ {len(downloaded_files) - 10} ä¸ªæ–‡ä»¶")
        else:
            print("âš ï¸  å°æ–‡ä»¶ä¸‹è½½å¯èƒ½ä¸å®Œæ•´ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
    else:
        print("æ²¡æœ‰å°æ–‡ä»¶éœ€è¦ä¸‹è½½")
    
    # æ­¥éª¤3: ç”Ÿæˆå¤§æ–‡ä»¶é“¾æ¥
    print("\n" + "=" * 70)
    print("æ­¥éª¤3: ç”Ÿæˆå¤§æ–‡ä»¶ä¸‹è½½é“¾æ¥")
    print("=" * 70)
    
    if big_files:
        total_big_size = sum(f["size"] for f in big_files)
        print(f"æ£€æµ‹åˆ° {len(big_files)} ä¸ªå¤§æ–‡ä»¶ï¼Œæ€»å¤§å°: {format_file_size(total_big_size)}")
        
        link_file = generate_big_file_links(repo_id, repo_name, big_files, repo_folder, subfolder)
    else:
        print("âœ… æ²¡æœ‰å¤§æ–‡ä»¶éœ€è¦ç”Ÿæˆé“¾æ¥")
    
    # æ­¥éª¤4: ç¼“å­˜æ¸…ç†
    ask_for_cache_cleanup(repo_folder)
    
    # å®Œæˆæç¤º
    print("\n" + "=" * 70)
    print("ä¸‹è½½ä»»åŠ¡å®Œæˆ!".center(70))
    print("=" * 70)
    
    print(f"\nğŸ“‚ æ–‡ä»¶å¤¹ä½ç½®: {repo_folder}")
    
    # æ˜¾ç¤ºæ–‡ä»¶å¤¹å†…å®¹ç»Ÿè®¡
    print(f"\nğŸ“Š æ–‡ä»¶å¤¹å†…å®¹ç»Ÿè®¡:")
    try:
        file_count = 0
        dir_count = 0
        total_size = 0
        
        for root, dirs, files in os.walk(repo_folder):
            dir_count += len(dirs)
            for file in files:
                if file.endswith('.txt') and 'å¤§æ–‡ä»¶' in file:
                    continue  # ä¸ç»Ÿè®¡é“¾æ¥æ–‡ä»¶
                try:
                    file_path = os.path.join(root, file)
                    total_size += os.path.getsize(file_path)
                    file_count += 1
                except:
                    pass
        
        print(f"   æ–‡ä»¶æ•°é‡: {file_count} ä¸ª")
        print(f"   æ–‡ä»¶å¤¹æ•°é‡: {dir_count} ä¸ª")
        print(f"   æ€»å¤§å°: {format_file_size(total_size)}")
    except:
        pass
    
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print("   1. å°æ–‡ä»¶å·²ä¸‹è½½åˆ°ä¸Šè¿°æ–‡ä»¶å¤¹")
    
    if big_files:
        print("   2. å¤§æ–‡ä»¶é“¾æ¥å·²ç”Ÿæˆåˆ°txtæ–‡ä»¶ä¸­")
        print("   3. è¯·ä½¿ç”¨ä¸‹è½½å·¥å…·ï¼ˆIDMã€è¿…é›·ç­‰ï¼‰ä¸‹è½½å¤§æ–‡ä»¶")
    
    print("\n" + "=" * 70)
    
    input("\næŒ‰å›è½¦é”®é€€å‡ºç¨‹åº...")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­æ“ä½œï¼Œç¨‹åºé€€å‡º")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        input("æŒ‰å›è½¦é”®é€€å‡º...")
        sys.exit(1)